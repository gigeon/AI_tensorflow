Relu
  1. Logistic regression: 분류(ex: 성공, 실패)
    성공 실패값을 계산해 그래프를 그려 성공 실패를 구분
    but 튀는 값이 나올 시 그래프가 이상해져 정확히 구분하기 어렵다는 리스크
  2. Sigmoid: 위 방법에 문제점을 해결하기 위해 나온 방법
  3.Vanishing gradi: 위 방법이역전파시 갈수록 값이 제대로 전달되어 지지 않음
    ReLU(x)함수사용
Dropout: overfitting(학습을 많이 시킬수록 무조건 정확해 지지 않음)을 보완하기 위해 불필요한
         신경망 부분을 제거
Softmax: 연산된 값을 0과1사이 값으로 변경해 나온 값중 가장 큰 값이 정답 
